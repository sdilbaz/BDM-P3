{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "#specifying the data path\n",
    "data_path = '/Users/hamidsakhi/git/BDM-P3/'\n",
    "\n",
    "\n",
    "purchase_file_path = data_path + 'purchase.csv'\n",
    "purchase_df = spark.read.format('csv').option('header','false').load(purchase_file_path)\n",
    "#since the dataset doesn't have column names we should add headers here\n",
    "purchase_df = purchase_df.withColumnRenamed('_c0','TransId')\\\n",
    "                               .withColumnRenamed('_c1','CustId')\\\n",
    "                               .withColumnRenamed('_c2','TransTotal')\\\n",
    "                               .withColumnRenamed('_c3','TransNumItem')\\\n",
    "                               .withColumnRenamed('_c4','TransDesc')\n",
    "purchase_df.createOrReplaceTempView('purchase')\n",
    "\n",
    "customer_file_path = data_path + 'customer.csv'\n",
    "customer_df = spark.read.format('csv').option('header','false').load(customer_file_path)\n",
    "#since the dataset doesn't have column names we should add headers here\n",
    "customer_df = customer_df.withColumnRenamed('_c0','ID')\\\n",
    "                               .withColumnRenamed('_c1','Name')\\\n",
    "                               .withColumnRenamed('_c2','Age')\\\n",
    "                               .withColumnRenamed('_c3','CountryCode')\\\n",
    "                               .withColumnRenamed('_c4','Salary')\n",
    "#creating the view \n",
    "customer_df.createOrReplaceTempView('customer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "T1 = spark.sql(\"\"\"SELECT * FROM purchase where transtotal>600\"\"\")\n",
    "\n",
    "purchase_df.createOrReplaceTempView('T1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "T2 = spark.sql(\"\"\" SELECT AVG(TransTotal) as Mean, MIN(TransTotal) As Min, Max(TransTotal) As Max \n",
    "                    FROM T1\n",
    "                    GROUP BY TransNumitem\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+------+\n",
      "|              Mean|  Min|   Max|\n",
      "+------------------+-----+------+\n",
      "|1004.8838597354422| 10.0|999.99|\n",
      "|1004.5357413229322| 10.0|999.99|\n",
      "|1004.0483739908203| 10.0|999.99|\n",
      "|1004.8273086510765| 10.0|999.99|\n",
      "| 1004.497867026794|10.03|999.99|\n",
      "|1004.5615556615148|10.01|999.99|\n",
      "|1005.7739147978656|10.01|999.99|\n",
      "|1004.8441406029958| 10.0|999.99|\n",
      "|1004.9988513081908| 10.0|999.97|\n",
      "|1004.5041289494897|10.01|999.99|\n",
      "|1005.1305228108343| 10.0|999.99|\n",
      "|1004.8712438917504| 10.0|999.99|\n",
      "|1006.6067367065345|10.01|999.99|\n",
      "|1006.5836723757462| 10.0|999.99|\n",
      "|1004.6530546314438| 10.0|999.98|\n",
      "+------------------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#reporting back\n",
    "T2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+--------+\n",
      "|custid|total_transnumItem|max(age)|\n",
      "+------+------------------+--------+\n",
      "|  1159|             767.0|      18|\n",
      "| 14887|            1007.0|      23|\n",
      "| 14899|             770.0|      19|\n",
      "|  1572|             900.0|      18|\n",
      "| 16576|             881.0|      22|\n",
      "| 17427|             870.0|      20|\n",
      "| 18726|             693.0|      19|\n",
      "| 19132|             914.0|      20|\n",
      "| 20512|             788.0|      21|\n",
      "| 25969|             896.0|      24|\n",
      "| 28316|             763.0|      23|\n",
      "| 29539|             752.0|      18|\n",
      "| 29573|             651.0|      19|\n",
      "| 30923|             785.0|      21|\n",
      "| 31518|             820.0|      21|\n",
      "| 31713|             910.0|      22|\n",
      "| 33783|             835.0|      20|\n",
      "| 35844|             774.0|      20|\n",
      "| 36526|             851.0|      18|\n",
      "| 40874|             785.0|      25|\n",
      "+------+------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "T3 = spark.sql(\"\"\" SELECT p.custid, sum(p.TransNumItem) as total_transnumItem, Max(c.age) FROM T1 p, customer c \n",
    "                    where p.custid = c.id\n",
    "                    and c.age between 18 and 25\n",
    "                    group by p.custid\n",
    "                    \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T4 = spark.sql(\"SELECT custid as c1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
